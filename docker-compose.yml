version: '3.7'

services:

  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    depends_on:
      - zookeeper

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
      - HDFS_CONF_dfs_replication=2
      - HDFS_CONF_dfs_namenode_safemode_threshold_pct=0.0
    ports:
      - "9870:9870"
      - "9000:9000"
    volumes:
      - ./hadoop/namenode:/hadoop/dfs/name
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 30s
      timeout: 10s
      retries: 5

  hadoop-datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
      - HDFS_CONF_dfs_replication=2
    ports:
      - "9864:9864"
    depends_on:
      hadoop-namenode:
        condition: service_healthy

  hadoop-datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    environment:
      - CLUSTER_NAME=bigdata-cluster
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
      - HDFS_CONF_dfs_replication=2
    ports:
      - "9865:9865"
    depends_on:
      hadoop-namenode:
        condition: service_healthy

  spark:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=client
      - SPARK_MASTER_URL=local
      - SPARK_EXTRA_CLASSPATH=/opt/bitnami/spark/jars
      - CORE_CONF_fs_defaultFS=hdfs://hadoop-namenode:9000
    depends_on:
      - hadoop-namenode
    command: sleep infinity
    volumes:
      - ./spark:/app
    working_dir: /app
    tty: true
